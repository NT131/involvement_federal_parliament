{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5635cd0d-99d7-49dc-b722-248fa18e74f5",
   "metadata": {},
   "source": [
    "# Dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68f6c1-564b-422e-9331-8129f0e5e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate 'keys' (i.e. when exact same position held by multiple people\n",
    "# Find duplicate keys\n",
    "# duplicate_keys = [key for key, count in Counter(minister_competences_2_names_dict).items() if count > 1]\n",
    "# duplicate_keys\n",
    "# Convert dictionary keys into a list\n",
    "keys_list = list(minister_competences_2_names_dict.keys())\n",
    "\n",
    "# Find duplicate keys using a set comprehension\n",
    "duplicate_keys = {key for key in keys_list if keys_list.count(key) > 1}\n",
    "duplicate_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898725b7-06b4-4c21-a8c8-cbb4fedd94f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8096a44d-5278-4eb6-87f4-f2b7719c16bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = questions_df.loc[0][\"Parlementslid\"]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79410d90-dfab-4cda-acea-826c3b499119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_author(input_string):\n",
    "    \"\"\"\n",
    "    Function to split strings as obtained from html page into name of member, his/her party and id of question\n",
    "\n",
    "    e.g. \n",
    "    'Anneleen\\n      Van Bossuyt,\\n      N-VA (07354)'\n",
    "    ('Anneleen Van Bossuyt', ' N-VA ', '07354')\n",
    "    \"\"\"\n",
    "    # Remove unnecessary characters (newlines and bracket at end (of id number))\n",
    "    cleaned_string = input_string.replace(\"\\n\", \"\").replace(\"\\)\", \"\")\n",
    "    \n",
    "    # Replace any sequence of spaces with a single space\n",
    "    cleaned_string = re.sub(r'\\s+', ' ', input_string)\n",
    "\n",
    "    # split on comma (between name and party) and left bracket (between party and id number)\n",
    "    name, party, id_number = re.split(r',|\\(', cleaned_string)\n",
    "\n",
    "    return name, party, id_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072eba07-c46d-441b-a27e-4e37602bbd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "name, party, id_nr = split_author(temp)\n",
    "name, party, id_nr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e37145-b115-49ef-8db3-bca595600bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f76d7c2-3b91-457c-87e6-b3df70f842d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7540db-6110-4b55-a5ab-a936b673b12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = scrape_bulletin(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ae776a-1b00-4c6b-93f0-844b332d5504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51287bb9-8b23-4c7a-bab4-f8f7ed301251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functiont to extract various elements stored in Beautiful Soup tags\n",
    "def extract_question_info(question_soup):\n",
    "    author = question_soup.find('td', text='Auteur').find_next('td').text.strip()\n",
    "    department = question_soup.find('td', text='Departement').find_next('td').text.strip()\n",
    "    title = question_soup.find('td', text='Titel').find_next('td').text.strip()\n",
    "    date_requested = question_soup.find('td', text='Datum indiening').find_next('td').text.strip()\n",
    "    answer_published = question_soup.find('td', text='Antwoord gepubliceerd').find_next('a')['href']\n",
    "    \n",
    "    return {\n",
    "        'Auteur': author,\n",
    "        'Departement': department,\n",
    "        'Title': title,\n",
    "        'Datum indiening': date_requested,\n",
    "        'Antwoord gepubliceerd': answer_published\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63fa94-25ac-42d2-a26f-fd46c1d0051b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e4f8ca-a39f-406e-8eeb-ccca8c641eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de7e04-38dd-453b-99a2-c1ea5648200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_b125 = \"https://www.dekamer.be/kvvcr/showpage.cfm?&language=nl&cfm=/site/wwwcfm/qrva/qrvatoc.cfm?legislat=55&bulletin=B125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dcb08-2294-4f95-9f56-9a489ab8af6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url_b125)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# destination_folder = \"pdfs\"\n",
    "# os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "question_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161b022-5e29-44b0-82f0-7f34d01ca664",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da424ab-2b53-4682-8458-86fedbb476e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data = []\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].startswith('showpage.cfm?section=qrva&language=nl&cfm=qrvaXml.cfm?legislat='):'\n",
    "        question_url = f\"https://www.dekamer.be/kvvcr/{link['href']}\"\n",
    "        question_response = requests.get(question_url)\n",
    "        question_soup = BeautifulSoup(question_response.text, 'html.parser')\n",
    "        \n",
    "        question_info = extract_question_info(question_soup)\n",
    "        question_data.append(question_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583feb2-7862-4447-9e40-deb2fce99978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WORKABLE CODE ##\n",
    "\n",
    "# def scrape_bulletin(url):\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         question_containers_0 = soup.find_all('div', class_='linklist_0')\n",
    "#         question_containers_1 = soup.find_all('div', class_='linklist_1')\n",
    "\n",
    "#         entries = []\n",
    "        \n",
    "#         for question_container in question_containers_0 + question_containers_1:\n",
    "#         # for question_container in question_containers_1:\n",
    "#             # title_element = question_container.find('a', class_='question-hyperlink')\n",
    "#             # author_element = question_container.find('div', class_='user-details').a\n",
    "#             # views_element = question_container.find('div', class_='views')\n",
    "\n",
    "#             tr_elements = question_container.find_all('tr')\n",
    "#             print(\"----\", tr_elements)\n",
    "\n",
    "#             # Initialize variables for additional information\n",
    "#             author, department, title, date_questions = \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "#             for tr_element in tr_elements:\n",
    "#                 td_elements = tr_element.find_all('td', class_='txt')\n",
    "                \n",
    "#                 if len(td_elements) == 2:\n",
    "#                     label, value = td_elements[0].text.strip(), td_elements[1].text.strip()\n",
    "\n",
    "#                     if \"Auteur\" in label:\n",
    "#                         author = value\n",
    "#                     elif \"Departement\" in label:\n",
    "#                         department = value\n",
    "#                     elif \"Titel\" in label:\n",
    "#                         title = value\n",
    "#                     elif \"Datum indiening\" in label:\n",
    "#                         date_questions = value\n",
    "#                     elif \"Antwoord gepubliceerd\" in label:\n",
    "#                         # Extract the URL if available\n",
    "#                         answer_published = td_elements[1].find('a')['href'] if td_elements[1].find('a') else \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#             # # Print or store the extracted information\n",
    "#             # print(f\"Auteur: {author}\")\n",
    "#             # print(f\"Departement: {department}\")\n",
    "#             # print(f\"Titel: {title}\")\n",
    "#             # print(f\"Datum vraag: {date_questions}\")\n",
    "#             # print(\"----\")\n",
    "\n",
    "#             if not author == department == title == date_questions == 'N/A':\n",
    "#                 entries.append([author, department, title, date_questions])\n",
    "\n",
    "#         return entries\n",
    "                \n",
    "\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1a8da-446d-4645-abc4-f9e55aa8f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKABLE CODE ##\n",
    "\n",
    "def scrape_bulletin(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        question_containers_0 = soup.find_all('div', class_='linklist_0')\n",
    "        question_containers_1 = soup.find_all('div', class_='linklist_1')\n",
    "\n",
    "        entries = []\n",
    "        \n",
    "        for question_container in question_containers_0 + question_containers_1:\n",
    "        # for question_container in question_containers_1:\n",
    "            # title_element = question_container.find('a', class_='question-hyperlink')\n",
    "            # author_element = question_container.find('div', class_='user-details').a\n",
    "            # views_element = question_container.find('div', class_='views')\n",
    "\n",
    "            tr_elements = question_container.find_all('tr')\n",
    "            print(\"----\", tr_elements)\n",
    "\n",
    "            # Initialize variables for additional information\n",
    "            author, department, title, date_questions = \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "            for tr_element in tr_elements:\n",
    "\n",
    "                # print(tr_element)\n",
    "                # print(\"**********\")\n",
    "                \n",
    "                td_elements = tr_element.find_all('td', class_='txt')\n",
    "\n",
    "                \n",
    "                \n",
    "                if len(td_elements) == 2:\n",
    "                    label, value = td_elements[0].text.strip(), td_elements[1].text.strip()\n",
    "\n",
    "                    # print(label)\n",
    "                    # print(\"****\")\n",
    "                    # print(value)\n",
    "                    # print(\"***************\")\n",
    "\n",
    "                    if \"Auteur\" in label:\n",
    "                        author = value\n",
    "                    elif \"Departement\" in label:\n",
    "                        department = value\n",
    "                    elif \"Titel\" in label:\n",
    "                        title = value\n",
    "                    elif \"Datum indiening\" in label:\n",
    "                        date_questions = value\n",
    "                    elif \"Antwoord gepubliceerd\" in label:\n",
    "                        # Extract the URL if available\n",
    "                        answer_published = td_elements[1].find('a')['href'] if td_elements[1].find('a') else \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # # Print or store the extracted information\n",
    "            # print(f\"Auteur: {author}\")\n",
    "            # print(f\"Departement: {department}\")\n",
    "            # print(f\"Titel: {title}\")\n",
    "            # print(f\"Datum vraag: {date_questions}\")\n",
    "            # print(\"----\")\n",
    "\n",
    "            if not author == department == title == date_questions == 'N/A':\n",
    "                entries.append([author, department, title, date_questions])\n",
    "\n",
    "        return entries\n",
    "                \n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac1d06-db73-498b-b931-4990461cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = scrape_bulletin(url_b125)\n",
    "# entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa243267-bff1-4b32-a38f-1d971abf7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f44038-ccce-4bc9-89da-f7c07d8e892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_bulletin(url):\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#         table_containers = soup.find_all('table', width=\"100%\")\n",
    "\n",
    "#         entries = []\n",
    "        \n",
    "#         for table_container in table_containers:\n",
    "#             print(\"***\", table_container)\n",
    "#             tr_elements = table_container.find_all('tr')\n",
    "#             # print(tr_elements)\n",
    "\n",
    "#     #         # Initialize variables for additional information\n",
    "#     #         author, department, title, date_questions, answer_published = \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "#     #         for tr_element in tr_elements:\n",
    "#     #             td_elements = tr_element.find_all('td', class_='txt')\n",
    "\n",
    "#     #             if len(td_elements) == 2:\n",
    "#     #             # if len(tr_element) == 2:\n",
    "#     #                 label, value = td_element[0].i.text.strip(), td_element[1].text.strip()\n",
    "\n",
    "#     #                 if \"Auteur\" in label:\n",
    "#     #                     author = value\n",
    "#     #                 elif \"Departement\" in label:\n",
    "#     #                     department = value\n",
    "#     #                 elif \"Titel\" in label:\n",
    "#     #                     title = value\n",
    "#     #                 elif \"Datum indiening\" in label:\n",
    "#     #                     date_match = re.search(r'\\d{2}/\\d{2}/\\d{4}', value)\n",
    "#     #                     date_questions = date_match.group(0) if date_match else \"N/A\"\n",
    "\n",
    "#     #                 elif \"Antwoord gepubliceerd\" in label:\n",
    "#     #                     answer_url_match = re.search(r'href=\"([^\"]+)\"', value)\n",
    "#     #                     answer_published = answer_url_match.group(1) if answer_url_match else \"N/A\"\n",
    "\n",
    "#     #         entries.append([author, department, title, date_questions, answer_published])\n",
    "\n",
    "\n",
    "#     #         if not author == department == title == date_questions == 'N/A':\n",
    "#     #             entries.append([author, department, title, date_questions, answer_published])\n",
    "\n",
    "#     #     return entries\n",
    "                \n",
    "#     # else:\n",
    "#     #     print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b026a8-026b-48d4-a66e-fec6addc5d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b930ba5-015e-4eb8-b48c-5464c3ba54ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdd4ec-050c-49c0-90fb-55495aba8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bulletin(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table_containers = soup.find_all('table')\n",
    "\n",
    "        print(\"*********\", table_containers)\n",
    "\n",
    "        # entries = []\n",
    "        \n",
    "        # for table_container in table_containers:\n",
    "        #     print(\"***\", table_container)\n",
    "        #     tr_elements = table_container.find_all('tr')\n",
    "            # print(tr_elements)\n",
    "\n",
    "    #         # Initialize variables for additional information\n",
    "    #         author, department, title, date_questions, answer_published = \"N/A\", \"N/A\", \"N/A\", \"N/A\", \"N/A\"\n",
    "\n",
    "    #         for tr_element in tr_elements:\n",
    "    #             td_elements = tr_element.find_all('td', class_='txt')\n",
    "\n",
    "    #             if len(td_elements) == 2:\n",
    "    #             # if len(tr_element) == 2:\n",
    "    #                 label, value = td_element[0].i.text.strip(), td_element[1].text.strip()\n",
    "\n",
    "    #                 if \"Auteur\" in label:\n",
    "    #                     author = value\n",
    "    #                 elif \"Departement\" in label:\n",
    "    #                     department = value\n",
    "    #                 elif \"Titel\" in label:\n",
    "    #                     title = value\n",
    "    #                 elif \"Datum indiening\" in label:\n",
    "    #                     date_match = re.search(r'\\d{2}/\\d{2}/\\d{4}', value)\n",
    "    #                     date_questions = date_match.group(0) if date_match else \"N/A\"\n",
    "\n",
    "    #                 elif \"Antwoord gepubliceerd\" in label:\n",
    "    #                     answer_url_match = re.search(r'href=\"([^\"]+)\"', value)\n",
    "    #                     answer_published = answer_url_match.group(1) if answer_url_match else \"N/A\"\n",
    "\n",
    "    #         entries.append([author, department, title, date_questions, answer_published])\n",
    "\n",
    "\n",
    "    #         if not author == department == title == date_questions == 'N/A':\n",
    "    #             entries.append([author, department, title, date_questions, answer_published])\n",
    "\n",
    "    #     return entries\n",
    "                \n",
    "    # else:\n",
    "    #     print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ba5bb-36ff-4fa1-9693-a0a27592b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a0217-389b-4193-b2ff-8d321d17923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = scrape_bulletin(url_b125)\n",
    "# entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423929f7-5aba-47a8-847c-aa7dad94987b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ad1d0-1ae0-438e-a4a5-db5dbba53385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bulletin(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html_code = response.text\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all elements that start with \"<td valign=\"middle\">\"\n",
    "        td_elements = soup.find_all('td', {'valign': 'middle'})\n",
    "        \n",
    "        # Iterate through each <td> element\n",
    "        for i in range(len(td_elements) - 1):\n",
    "            start_index = html_code.find(str(td_elements[i]))\n",
    "            end_index = html_code.find(str(td_elements[i + 1]))\n",
    "        \n",
    "            # Extract the HTML content between start_index and end_index\n",
    "            extracted_html = html_code[start_index:end_index]\n",
    "        \n",
    "            # Print or further process the extracted HTML content\n",
    "            print(extracted_html)\n",
    "\n",
    "            print(\"*********\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c57dd-3bcf-4f92-b687-0015f6163809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bulletin(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\n",
    "        # Find all the tables with class 'txt'\n",
    "        tables = soup.find_all('table', class_='txt')\n",
    "        \n",
    "        # Loop through each table and extract the required information\n",
    "        for index, table in enumerate(tables):\n",
    "            # Extract data from the table\n",
    "            row_data = table.find_all('tr')\n",
    "            \n",
    "            # Extract specific elements from each row\n",
    "            for row in row_data:\n",
    "                # Extract the text inside the <td> tags\n",
    "                cells = row.find_all('td')\n",
    "                # cells = row.find_all('td', class_='txt')\n",
    "\n",
    "                # cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "                # cell_texts = [text.replace('\\xa0', ' ') for text in cell_texts]  # Replace non-breaking space with regular space\n",
    "                \n",
    "                # Print the extracted data\n",
    "                print(\"\".join(cells))\n",
    "            \n",
    "            # Add asterisks to indicate the end of each section\n",
    "            print('*' * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faecdc6-e84c-4324-8706-6804754179fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_bulletin(url_b125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4da56a1-3442-4a38-a1c4-cb098f0301b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_information_from_url(url):\n",
    "    # Fetch HTML content from the provided URL\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Failed to fetch the page. Please check the URL.\")\n",
    "        return\n",
    "\n",
    "    html_code = response.text\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    # Extracting information from each entry\n",
    "    tables = soup.find_all('table', class_='txt')\n",
    "\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "\n",
    "        # Extracting information from each row\n",
    "        for row in rows:\n",
    "            # Check if the row contains relevant information\n",
    "            if row.find('i', text='Auteur'):\n",
    "                author = row.find('td', class_='txt').get_text(strip=True)\n",
    "                department = row.find_next('tr').find('td', class_='txt').get_text(strip=True)\n",
    "                title = row.find_next('tr').find('td', class_='txt').get_text(strip=True)\n",
    "                date_submission = row.find('i', text='Datum indiening').find_next('td', class_='txt').get_text(strip=True)\n",
    "                answer_published_element = row.find('i', text='Antwoord gepubliceerd')\n",
    "                answer_published = answer_published_element.find_next('a').get('href') if answer_published_element else \"N/A\"\n",
    "\n",
    "                # Print or use the extracted information as needed\n",
    "                print(\"Auteur:\", author)\n",
    "                print(\"Departement:\", department)\n",
    "                print(\"Titel:\", title)\n",
    "                print(\"Datum indiening:\", date_submission)\n",
    "                print(\"Antwoord gepubliceerd:\", answer_published)\n",
    "                print(\"\\n\")\n",
    "\n",
    "# Example usage with a URL\n",
    "url = \"https://example.com/your_page\"\n",
    "extract_information_from_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c36581d-2642-4594-96a6-1070a9aa61f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63295d5a-ad98-4ec3-8392-6c362327a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_legislative_questions(url):\n",
    "#     # Send a GET request to the URL\n",
    "#     response = requests.get(url)\n",
    "    \n",
    "#     # Check if the request was successful (status code 200)\n",
    "#     if response.status_code == 200:\n",
    "#         # Parse the HTML content using BeautifulSoup\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "#         # Find the h4 element with the specific text as an anchor\n",
    "#         # anchor_h4 = soup.find('h4', text='Schriftelijke vragen en antwoorden Nr B125 - Zittingsperiode : 55')\n",
    "#         anchor_h4 = soup.find('h4', text='Schriftelijke vragen en antwoorden Nr B125 - Zittingsperiode : 55')\n",
    "        \n",
    "#         if anchor_h4:\n",
    "#             # Find the parent element of the anchor_h4 to locate the container\n",
    "#             container_div = anchor_h4.find_parent('div')\n",
    "            \n",
    "#             # Find all table rows (tr) within the container_div\n",
    "#             legislative_items = container_div.find_all('tr')\n",
    "            \n",
    "#             # Iterate through each legislative item and extract information\n",
    "#             for item in legislative_items:\n",
    "#                 # Extract reference number (B125) from the link\n",
    "#                 reference_number = item.find('a', {'href': lambda x: x and 'dossierID' in x}).text.strip()\n",
    "                \n",
    "#                 # Extract author, department, title, and date information\n",
    "#                 author = item.find('td', {'class': 'txt', 'width': '150'}).find_next('td', {'class': 'txt'}).text.strip()\n",
    "#                 department = item.find('i', text='Departement').find_next('td', {'class': 'txt'}).text.strip()\n",
    "#                 title = item.find('i', text='Titel').find_next('td', {'class': 'txt'}).text.strip()\n",
    "#                 date = item.find('i', text='Datum indiening').find_next('td', {'class': 'txt'}).text.strip()\n",
    "                \n",
    "#                 # Extract the link to the published answer\n",
    "#                 answer_link = item.find('a', {'target': '_blank'}).get('href')\n",
    "                \n",
    "#                 # Print or store the extracted information\n",
    "#                 print(f\"Reference Number: {reference_number}\")\n",
    "#                 print(f\"Author: {author}\")\n",
    "#                 print(f\"Department: {department}\")\n",
    "#                 print(f\"Title: {title}\")\n",
    "#                 print(f\"Date of Submission: {date}\")\n",
    "#                 print(f\"Answer Link: {answer_link}\")\n",
    "#                 print(\"------------------------------\")\n",
    "#         else:\n",
    "#             print(\"Error: Anchor h4 not found.\")\n",
    "\n",
    "#     else:\n",
    "#         print(f\"Error: Unable to fetch the page. Status code: {response.status_code}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62129c-d7f8-4f12-8fe8-8313c5d658d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a sample URL\n",
    "parse_legislative_questions(url_b125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d1410e-77fa-4169-9a01-5e2e646b81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(url):\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     destination_folder = \"pdfs\"\n",
    "#     os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "#     question_data = []\n",
    "\n",
    "#     for link in soup.find_all('a', href=True):\n",
    "#         if link['href'].startswith('showpage.cfm?section=qrva&language=nl&cfm=qrvaXml.cfm?legislat='):\n",
    "#             question_url = f\"https://www.dekamer.be/kvvcr/{link['href']}\"\n",
    "#             question_response = requests.get(question_url)\n",
    "#             question_soup = BeautifulSoup(question_response.text, 'html.parser')\n",
    "            \n",
    "#             question_info = extract_question_info(question_soup)\n",
    "#             question_data.append(question_info)\n",
    "\n",
    "#     # Create a DataFrame from the extracted data\n",
    "#     df = pd.DataFrame(question_data)\n",
    "\n",
    "#     # # Save the DataFrame to a CSV file\n",
    "#     # df.to_csv('question_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518d175c-07b3-4bf7-8101-beab846657e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_b125 = \"https://www.dekamer.be/kvvcr/showpage.cfm?&language=nl&cfm=/site/wwwcfm/qrva/qrvatoc.cfm?legislat=55&bulletin=B125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e157ea-ff93-49c6-b269-868c27cd3c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, destination_folder):\n",
    "    response = requests.get(pdf_url)\n",
    "    filename = os.path.join(destination_folder, os.path.basename(pdf_url))\n",
    "    with open(filename, 'wb') as pdf_file:\n",
    "        pdf_file.write(response.content)\n",
    "    return filename\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc[page_num]\n",
    "        text += page.get_text()\n",
    "    doc.close()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    url = \"https://www.dekamer.be/kvvcr/showpage.cfm?section=/qrva&language=nl&cfm=qrvaList.cfm\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    destination_folder = \"pdfs\"\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "    pdf_data = []\n",
    "\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            pdf_url = link['href']\n",
    "            pdf_path = download_pdf(pdf_url, destination_folder)\n",
    "            text_content = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            pdf_data.append({\n",
    "                'PDF_URL': pdf_url,\n",
    "                'TEXT_CONTENT': text_content\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    df = pd.DataFrame(pdf_data)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('pdf_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
